# Transformer Models for Voltage Prediction

This repository provides the trained Transformer-based models, and interpretability outputs for **voltage prediction of battery electrode materials**. It includes baseline metrics, per-ion analysis with weighted sampling, and SHAP-based feature attribution.

---


## Repository Structure

```text
baseline_metrics_and_bootstrapping/  
â”œâ”€â”€ entire_model_transformer_*.pth  
â”œâ”€â”€ sd_transformer_*.ipynb        # Baseline & bootstrapping notebooks  

per-ion-metrics/
â”œâ”€â”€ entire_model_transformer_*.pth  
â”œâ”€â”€ *_weighted_sampling.ipynb     # Weighted-sample training for rare ions  
â”œâ”€â”€ base.ipynb  
â”œâ”€â”€ ffn.ipynb  
â”œâ”€â”€ rnn_gru.ipynb  
â”œâ”€â”€ rnn_lstm.ipynb  

shap/
â”œâ”€â”€ Base_Transformer_global.ipynb  
â”œâ”€â”€ FNN_global.ipynb  
â”œâ”€â”€ GRU_global.ipynb  
â”œâ”€â”€ LSTM_local.ipynb  

```


## ðŸ“‘ Contents
 
- **Trained Models**: Saved PyTorch `.pth` files for:  
  - Base Transformer  
  - Transformer + FFNN  
  - Transformer + GRU  
  - Transformer + LSTM  
- **Weighted-Sample Models**: Versions trained with loss weighting to improve per-ion accuracy on underrepresented ions (e.g., Rb, Y).  
- **Baseline Metrics & Bootstrapping**: Scripts for error metrics (MAE, MSE, RÂ²) and bootstrapped confidence intervals.  
- **Per-Ion Metrics**: Training and evaluation notebooks with weighted sampling.  
- **SHAP Interpretability**: Global and local SHAP analyses for feature importance and case-by-case explanations.  

---
