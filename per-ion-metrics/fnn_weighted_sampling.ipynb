{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb90b78-7b33-45da-8d0b-4fb4c41908a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import MSELoss\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce3d56-2a36-489c-a27e-3f9e8b7c6e54",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5178a4-cd1e-4970-b646-b8f24efff1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv('battery_feature_extracted.csv')\n",
    "X = dataset.drop(columns=['average_voltage'])\n",
    "y = dataset['average_voltage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db28c14-c76d-4cfe-8c40-4586f20a8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e808941-35f6-4db2-af30-56ce81a0a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe613f86-8291-4021-aada-9dfcf8a01847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).unsqueeze(1)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b444fcd-2b80-47f1-a55d-51b36a21f4ab",
   "metadata": {},
   "source": [
    "# Data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5afd65-d3f0-4112-b471-9935264b187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-sample weights\n",
    "ion_columns = [col for col in X_train.columns if col.startswith(\"working_ion_\")]\n",
    "ion_counts = X_train[ion_columns].sum()\n",
    "ion_weights = 1.0 / ion_counts\n",
    "ion_weights /= ion_weights.sum()\n",
    "train_weights = X_train[ion_columns].dot(ion_weights.astype(np.float32))\n",
    "train_weights_tensor = torch.tensor(train_weights.values.astype(np.float32)).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1966a3a-bb26-45ca-8130-eb11954eec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FeedForwardNN used within TabTransformer\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.residual = nn.Linear(input_size, output_size)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return self.alpha * x + (1 - self.alpha) * residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5b47ee-88d0-4976-b222-70f87d2bcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TabTransformer with FFNN\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, num_features, output_size=1, dim_embedding=128, num_heads=2, num_layers=2, ffnn_hidden_size=128):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(num_features, dim_embedding)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_embedding, nhead=num_heads, batch_first=True, dropout=0.7)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.ffnn = FeedForwardNN(dim_embedding, ffnn_hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.transformer(x)\n",
    "        x = x[:, 0, :]\n",
    "        return self.ffnn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f6d0c1-2b77-4499-bdc6-b8b6eff52ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted composite loss\n",
    "class WeightedCompositeLoss(nn.Module):\n",
    "    def forward(self, outputs, targets, weights):\n",
    "        mse = (weights * (outputs - targets) ** 2).mean()\n",
    "        mae = (weights * torch.abs(outputs - targets)).mean()\n",
    "        return mse + 0.5 * mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a33fe1c-fbdd-4f42-9d5b-862a5f75334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TabTransformer(num_features=X_train_tensor.shape[1]).to(device)\n",
    "criterion = WeightedCompositeLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00075)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "250ba7e2-0f59-4337-aeb1-6f5f9754269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move tensors to device\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "train_weights_tensor = train_weights_tensor.to(device)\n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0352f353-a5c4-4f69-a008-94b144b262a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.2693, Val Loss: 4.4267\n",
      "Epoch 10, Train Loss: 0.0917, Val Loss: 4.4085\n",
      "Epoch 20, Train Loss: 0.0907, Val Loss: 3.5020\n",
      "Epoch 30, Train Loss: 0.0890, Val Loss: 3.4655\n",
      "Epoch 40, Train Loss: 0.0794, Val Loss: 3.6511\n",
      "Epoch 50, Train Loss: 0.0572, Val Loss: 4.4903\n",
      "Epoch 60, Train Loss: 0.0491, Val Loss: 4.9404\n",
      "Epoch 70, Train Loss: 0.0437, Val Loss: 4.5306\n",
      "Epoch 80, Train Loss: 0.0394, Val Loss: 4.7564\n",
      "Epoch 90, Train Loss: 0.0329, Val Loss: 4.3306\n",
      "Epoch 100, Train Loss: 0.0301, Val Loss: 4.2787\n",
      "Epoch 110, Train Loss: 0.0267, Val Loss: 3.6967\n",
      "Epoch 120, Train Loss: 0.0266, Val Loss: 3.4371\n",
      "Epoch 130, Train Loss: 0.0232, Val Loss: 3.2921\n",
      "Epoch 140, Train Loss: 0.0209, Val Loss: 3.0072\n",
      "Epoch 150, Train Loss: 0.0207, Val Loss: 2.9618\n",
      "Epoch 160, Train Loss: 0.0195, Val Loss: 2.9172\n",
      "Epoch 170, Train Loss: 0.0179, Val Loss: 2.5621\n",
      "Epoch 180, Train Loss: 0.0175, Val Loss: 2.6360\n",
      "Epoch 190, Train Loss: 0.0169, Val Loss: 2.2799\n",
      "Epoch 200, Train Loss: 0.0159, Val Loss: 2.0698\n",
      "Epoch 210, Train Loss: 0.0154, Val Loss: 2.2028\n",
      "Epoch 220, Train Loss: 0.0156, Val Loss: 1.9806\n",
      "Epoch 230, Train Loss: 0.0143, Val Loss: 2.0931\n",
      "Epoch 240, Train Loss: 0.0143, Val Loss: 1.9482\n",
      "Epoch 250, Train Loss: 0.0143, Val Loss: 1.8063\n",
      "Epoch 260, Train Loss: 0.0139, Val Loss: 1.6382\n",
      "Epoch 270, Train Loss: 0.0139, Val Loss: 1.7651\n",
      "Epoch 280, Train Loss: 0.0130, Val Loss: 1.7342\n",
      "Epoch 290, Train Loss: 0.0129, Val Loss: 1.8782\n",
      "Epoch 300, Train Loss: 0.0122, Val Loss: 1.5223\n",
      "Epoch 310, Train Loss: 0.0123, Val Loss: 1.6177\n",
      "Epoch 320, Train Loss: 0.0125, Val Loss: 1.4648\n",
      "Epoch 330, Train Loss: 0.0120, Val Loss: 1.3971\n",
      "Epoch 340, Train Loss: 0.0120, Val Loss: 1.4563\n",
      "Epoch 350, Train Loss: 0.0125, Val Loss: 1.4154\n",
      "Epoch 360, Train Loss: 0.0114, Val Loss: 1.5719\n",
      "Epoch 370, Train Loss: 0.0114, Val Loss: 1.4133\n",
      "Epoch 380, Train Loss: 0.0112, Val Loss: 1.3975\n",
      "Epoch 390, Train Loss: 0.0107, Val Loss: 1.4029\n",
      "Epoch 400, Train Loss: 0.0115, Val Loss: 1.2632\n",
      "Epoch 410, Train Loss: 0.0109, Val Loss: 1.3779\n",
      "Epoch 420, Train Loss: 0.0104, Val Loss: 1.2990\n",
      "Epoch 430, Train Loss: 0.0105, Val Loss: 1.2249\n",
      "Epoch 440, Train Loss: 0.0102, Val Loss: 1.2522\n",
      "Epoch 450, Train Loss: 0.0099, Val Loss: 1.2255\n",
      "Epoch 460, Train Loss: 0.0097, Val Loss: 1.1427\n",
      "Epoch 470, Train Loss: 0.0093, Val Loss: 1.0646\n",
      "Epoch 480, Train Loss: 0.0095, Val Loss: 1.0917\n",
      "Epoch 490, Train Loss: 0.0092, Val Loss: 1.0749\n",
      "Epoch 500, Train Loss: 0.0086, Val Loss: 1.0406\n",
      "Epoch 510, Train Loss: 0.0089, Val Loss: 1.0165\n",
      "Epoch 520, Train Loss: 0.0081, Val Loss: 0.9783\n",
      "Epoch 530, Train Loss: 0.0079, Val Loss: 0.9837\n",
      "Epoch 540, Train Loss: 0.0079, Val Loss: 0.9184\n",
      "Epoch 550, Train Loss: 0.0079, Val Loss: 0.9590\n",
      "Epoch 560, Train Loss: 0.0070, Val Loss: 0.9471\n",
      "Epoch 570, Train Loss: 0.0069, Val Loss: 0.9345\n",
      "Epoch 580, Train Loss: 0.0066, Val Loss: 0.9065\n",
      "Epoch 590, Train Loss: 0.0067, Val Loss: 0.8798\n",
      "Epoch 600, Train Loss: 0.0063, Val Loss: 0.9613\n",
      "Epoch 610, Train Loss: 0.0058, Val Loss: 0.7965\n",
      "Epoch 620, Train Loss: 0.0052, Val Loss: 0.8316\n",
      "Epoch 630, Train Loss: 0.0047, Val Loss: 0.8670\n",
      "Epoch 640, Train Loss: 0.0044, Val Loss: 0.8192\n",
      "Epoch 650, Train Loss: 0.0048, Val Loss: 0.8958\n",
      "Epoch 660, Train Loss: 0.0042, Val Loss: 0.8461\n",
      "Epoch 670, Train Loss: 0.0038, Val Loss: 0.8229\n",
      "Epoch 680, Train Loss: 0.0035, Val Loss: 0.8309\n",
      "Epoch 690, Train Loss: 0.0031, Val Loss: 0.8311\n",
      "Epoch 700, Train Loss: 0.0033, Val Loss: 0.8694\n",
      "Epoch 710, Train Loss: 0.0032, Val Loss: 0.8532\n",
      "Epoch 720, Train Loss: 0.0031, Val Loss: 0.7992\n",
      "Epoch 730, Train Loss: 0.0028, Val Loss: 0.8762\n",
      "Epoch 740, Train Loss: 0.0028, Val Loss: 0.8673\n",
      "Epoch 750, Train Loss: 0.0028, Val Loss: 0.8175\n",
      "Epoch 760, Train Loss: 0.0026, Val Loss: 0.8246\n",
      "Epoch 770, Train Loss: 0.0027, Val Loss: 0.7900\n",
      "Epoch 780, Train Loss: 0.0025, Val Loss: 0.8082\n",
      "Epoch 790, Train Loss: 0.0023, Val Loss: 0.7904\n",
      "Epoch 800, Train Loss: 0.0024, Val Loss: 0.8007\n",
      "Epoch 810, Train Loss: 0.0024, Val Loss: 0.7765\n",
      "Epoch 820, Train Loss: 0.0023, Val Loss: 0.7935\n",
      "Epoch 830, Train Loss: 0.0021, Val Loss: 0.7481\n",
      "Epoch 840, Train Loss: 0.0023, Val Loss: 0.7611\n",
      "Epoch 850, Train Loss: 0.0021, Val Loss: 0.7774\n",
      "Epoch 860, Train Loss: 0.0019, Val Loss: 0.7433\n",
      "Epoch 870, Train Loss: 0.0021, Val Loss: 0.7900\n",
      "Epoch 880, Train Loss: 0.0024, Val Loss: 0.7276\n",
      "Epoch 890, Train Loss: 0.0021, Val Loss: 0.7394\n",
      "Epoch 900, Train Loss: 0.0018, Val Loss: 0.7622\n",
      "Epoch 910, Train Loss: 0.0022, Val Loss: 0.7421\n",
      "Epoch 920, Train Loss: 0.0019, Val Loss: 0.7602\n",
      "Epoch 930, Train Loss: 0.0023, Val Loss: 0.7473\n",
      "Epoch 940, Train Loss: 0.0018, Val Loss: 0.7465\n",
      "Epoch 950, Train Loss: 0.0020, Val Loss: 0.7929\n",
      "Epoch 960, Train Loss: 0.0018, Val Loss: 0.7363\n",
      "Epoch 970, Train Loss: 0.0016, Val Loss: 0.7796\n",
      "Epoch 980, Train Loss: 0.0019, Val Loss: 0.7324\n",
      "Epoch 990, Train Loss: 0.0019, Val Loss: 0.6945\n",
      "Epoch 1000, Train Loss: 0.0016, Val Loss: 0.7098\n",
      "Epoch 1010, Train Loss: 0.0017, Val Loss: 0.7040\n",
      "Epoch 1020, Train Loss: 0.0019, Val Loss: 0.6903\n",
      "Epoch 1030, Train Loss: 0.0015, Val Loss: 0.7083\n",
      "Epoch 1040, Train Loss: 0.0016, Val Loss: 0.7026\n",
      "Epoch 1050, Train Loss: 0.0016, Val Loss: 0.6971\n",
      "Epoch 1060, Train Loss: 0.0015, Val Loss: 0.6988\n",
      "Epoch 1070, Train Loss: 0.0014, Val Loss: 0.6852\n",
      "Epoch 1080, Train Loss: 0.0018, Val Loss: 0.7017\n",
      "Epoch 1090, Train Loss: 0.0014, Val Loss: 0.6830\n",
      "Epoch 1100, Train Loss: 0.0014, Val Loss: 0.6369\n",
      "Epoch 1110, Train Loss: 0.0014, Val Loss: 0.6592\n",
      "Epoch 1120, Train Loss: 0.0013, Val Loss: 0.6541\n",
      "Epoch 1130, Train Loss: 0.0015, Val Loss: 0.6672\n",
      "Epoch 1140, Train Loss: 0.0015, Val Loss: 0.6450\n",
      "Epoch 1150, Train Loss: 0.0017, Val Loss: 0.6611\n",
      "Epoch 1160, Train Loss: 0.0012, Val Loss: 0.6621\n",
      "Epoch 1170, Train Loss: 0.0012, Val Loss: 0.6531\n",
      "Epoch 1180, Train Loss: 0.0012, Val Loss: 0.6624\n",
      "Epoch 1190, Train Loss: 0.0012, Val Loss: 0.6773\n",
      "Epoch 1200, Train Loss: 0.0012, Val Loss: 0.6515\n",
      "Epoch 1210, Train Loss: 0.0012, Val Loss: 0.6402\n",
      "Epoch 1220, Train Loss: 0.0011, Val Loss: 0.6510\n",
      "Epoch 1230, Train Loss: 0.0012, Val Loss: 0.6450\n",
      "Epoch 1240, Train Loss: 0.0012, Val Loss: 0.6284\n",
      "Epoch 1250, Train Loss: 0.0014, Val Loss: 0.6800\n",
      "Epoch 1260, Train Loss: 0.0012, Val Loss: 0.6572\n",
      "Epoch 1270, Train Loss: 0.0011, Val Loss: 0.6471\n",
      "Epoch 1280, Train Loss: 0.0010, Val Loss: 0.6236\n",
      "Epoch 1290, Train Loss: 0.0011, Val Loss: 0.6364\n",
      "Epoch 1300, Train Loss: 0.0012, Val Loss: 0.6310\n",
      "Epoch 1310, Train Loss: 0.0012, Val Loss: 0.6296\n",
      "Epoch 1320, Train Loss: 0.0011, Val Loss: 0.6281\n",
      "Epoch 1330, Train Loss: 0.0015, Val Loss: 0.6504\n",
      "Epoch 1340, Train Loss: 0.0011, Val Loss: 0.6418\n",
      "Epoch 1350, Train Loss: 0.0010, Val Loss: 0.6261\n",
      "Epoch 1360, Train Loss: 0.0010, Val Loss: 0.6001\n",
      "Epoch 1370, Train Loss: 0.0012, Val Loss: 0.6039\n",
      "Epoch 1380, Train Loss: 0.0009, Val Loss: 0.6070\n",
      "Epoch 1390, Train Loss: 0.0009, Val Loss: 0.6125\n",
      "Epoch 1400, Train Loss: 0.0009, Val Loss: 0.6195\n",
      "Epoch 1410, Train Loss: 0.0009, Val Loss: 0.6063\n",
      "Epoch 1420, Train Loss: 0.0015, Val Loss: 0.6545\n",
      "Epoch 1430, Train Loss: 0.0012, Val Loss: 0.6428\n",
      "Epoch 1440, Train Loss: 0.0010, Val Loss: 0.6090\n",
      "Epoch 1450, Train Loss: 0.0010, Val Loss: 0.6002\n",
      "Epoch 1460, Train Loss: 0.0009, Val Loss: 0.6343\n",
      "Epoch 1470, Train Loss: 0.0010, Val Loss: 0.6181\n",
      "Epoch 1480, Train Loss: 0.0010, Val Loss: 0.6329\n",
      "Epoch 1490, Train Loss: 0.0010, Val Loss: 0.6207\n",
      "Epoch 1500, Train Loss: 0.0009, Val Loss: 0.6363\n",
      "Epoch 1510, Train Loss: 0.0011, Val Loss: 0.6194\n",
      "Epoch 1520, Train Loss: 0.0009, Val Loss: 0.6238\n",
      "Epoch 1530, Train Loss: 0.0010, Val Loss: 0.6331\n",
      "Epoch 1540, Train Loss: 0.0008, Val Loss: 0.6004\n",
      "Epoch 1550, Train Loss: 0.0012, Val Loss: 0.6177\n",
      "Epoch 1560, Train Loss: 0.0010, Val Loss: 0.6133\n",
      "Epoch 1570, Train Loss: 0.0012, Val Loss: 0.6094\n",
      "Epoch 1580, Train Loss: 0.0010, Val Loss: 0.6222\n",
      "Epoch 1590, Train Loss: 0.0010, Val Loss: 0.5919\n",
      "Epoch 1600, Train Loss: 0.0007, Val Loss: 0.6140\n",
      "Epoch 1610, Train Loss: 0.0009, Val Loss: 0.6192\n",
      "Epoch 1620, Train Loss: 0.0008, Val Loss: 0.6122\n",
      "Epoch 1630, Train Loss: 0.0008, Val Loss: 0.6194\n",
      "Epoch 1640, Train Loss: 0.0010, Val Loss: 0.6309\n",
      "Epoch 1650, Train Loss: 0.0008, Val Loss: 0.6346\n",
      "Epoch 1660, Train Loss: 0.0009, Val Loss: 0.6029\n",
      "Epoch 1670, Train Loss: 0.0012, Val Loss: 0.6204\n",
      "Epoch 1680, Train Loss: 0.0008, Val Loss: 0.6185\n",
      "Epoch 1690, Train Loss: 0.0008, Val Loss: 0.6014\n",
      "Epoch 1700, Train Loss: 0.0007, Val Loss: 0.6096\n",
      "Epoch 1710, Train Loss: 0.0008, Val Loss: 0.6285\n",
      "Epoch 1720, Train Loss: 0.0007, Val Loss: 0.6580\n",
      "Epoch 1730, Train Loss: 0.0009, Val Loss: 0.6209\n",
      "Epoch 1740, Train Loss: 0.0008, Val Loss: 0.6246\n",
      "Epoch 1750, Train Loss: 0.0008, Val Loss: 0.6141\n",
      "Epoch 1760, Train Loss: 0.0012, Val Loss: 0.6143\n",
      "Epoch 1770, Train Loss: 0.0007, Val Loss: 0.6244\n",
      "Epoch 1780, Train Loss: 0.0007, Val Loss: 0.6199\n",
      "Epoch 1790, Train Loss: 0.0010, Val Loss: 0.6170\n",
      "Epoch 1800, Train Loss: 0.0009, Val Loss: 0.6144\n",
      "Epoch 1810, Train Loss: 0.0009, Val Loss: 0.6335\n",
      "Epoch 1820, Train Loss: 0.0008, Val Loss: 0.6187\n",
      "Epoch 1830, Train Loss: 0.0007, Val Loss: 0.6073\n",
      "Epoch 1840, Train Loss: 0.0006, Val Loss: 0.6145\n",
      "Epoch 1850, Train Loss: 0.0008, Val Loss: 0.6089\n",
      "Epoch 1860, Train Loss: 0.0007, Val Loss: 0.6193\n",
      "Epoch 1870, Train Loss: 0.0006, Val Loss: 0.6310\n",
      "Epoch 1880, Train Loss: 0.0009, Val Loss: 0.6072\n",
      "Epoch 1890, Train Loss: 0.0008, Val Loss: 0.6081\n",
      "Epoch 1900, Train Loss: 0.0006, Val Loss: 0.6158\n",
      "Epoch 1910, Train Loss: 0.0007, Val Loss: 0.6065\n",
      "Epoch 1920, Train Loss: 0.0008, Val Loss: 0.6133\n",
      "Epoch 1930, Train Loss: 0.0007, Val Loss: 0.5967\n",
      "Epoch 1940, Train Loss: 0.0008, Val Loss: 0.5990\n",
      "Epoch 1950, Train Loss: 0.0008, Val Loss: 0.6252\n",
      "Epoch 1960, Train Loss: 0.0010, Val Loss: 0.6142\n",
      "Epoch 1970, Train Loss: 0.0007, Val Loss: 0.6201\n",
      "Epoch 1980, Train Loss: 0.0010, Val Loss: 0.6309\n",
      "Epoch 1990, Train Loss: 0.0009, Val Loss: 0.6146\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "training_losses, validation_losses = [], []\n",
    "for epoch in range(2000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    train_loss = criterion(output, y_train_tensor, train_weights_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val_tensor)\n",
    "        val_loss = criterion(val_output, y_val_tensor, torch.ones_like(y_val_tensor))\n",
    "\n",
    "    training_losses.append(train_loss.item())\n",
    "    validation_losses.append(val_loss.item())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f9d39b-cab0-4bb8-95ef-f90d03e1b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    mse = nn.MSELoss()(predictions, y_test_tensor).item()\n",
    "    mae = nn.L1Loss()(predictions, y_test_tensor).item()\n",
    "    ss_total = torch.sum((y_test_tensor - torch.mean(y_test_tensor))**2)\n",
    "    ss_residual = torch.sum((y_test_tensor - predictions)**2)\n",
    "    r2_score = 1 - (ss_residual / ss_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1ffd67-0f33-4d19-b22e-b43d9a5c2f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test MSE: 0.3485\n",
      "Test MAE: 0.3154\n",
      "Test R²: 0.8670\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nTest MSE: {mse:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')\n",
    "print(f'Test R²: {r2_score.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f52f297-42b9-463d-9bbf-ed67812a62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-ion metrics on test set:\n",
      "Al: MAE = 0.3064, MSE = 0.1806, R² = 0.8430\n",
      "Ca: MAE = 0.2114, MSE = 0.0864, R² = 0.9300\n",
      "Cs: MAE = 0.4337, MSE = 0.2710, R² = 0.2849\n",
      "K: MAE = 0.2114, MSE = 0.0642, R² = 0.9809\n",
      "Li: MAE = 0.3490, MSE = 0.3923, R² = 0.8152\n",
      "Mg: MAE = 0.4786, MSE = 0.9721, R² = 0.7242\n",
      "Na: MAE = 0.1529, MSE = 0.0414, R² = 0.9793\n",
      "Rb: MAE = 0.1458, MSE = 0.0272, R² = 0.9832\n",
      "Y: MAE = 0.3089, MSE = 0.1129, R² = 0.7595\n",
      "Zn: MAE = 0.2473, MSE = 0.1437, R² = 0.8392\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "X_test_df = X_test.reset_index(drop=True).copy()\n",
    "X_test_df['true'] = y_test.values\n",
    "X_test_df['pred'] = predictions.cpu().numpy().flatten()\n",
    "\n",
    "print(\"\\nPer-ion metrics on test set:\")\n",
    "for ion in ion_columns:\n",
    "    subset = X_test_df[X_test_df[ion] == 1]\n",
    "    if not subset.empty:\n",
    "        y_true = subset['true'].values\n",
    "        y_pred = subset['pred'].values\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"{ion.replace('working_ion_', '')}: MAE = {mae:.4f}, MSE = {mse:.4f}, R² = {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27f3a54a-64b9-4e9c-bba5-115680ba8b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-ion MAE on test set:\n",
      "Al: MAE = 0.3064\n",
      "Ca: MAE = 0.2114\n",
      "Cs: MAE = 0.4337\n",
      "K: MAE = 0.2114\n",
      "Li: MAE = 0.3490\n",
      "Mg: MAE = 0.4786\n",
      "Na: MAE = 0.1529\n",
      "Rb: MAE = 0.1458\n",
      "Y: MAE = 0.3089\n",
      "Zn: MAE = 0.2473\n"
     ]
    }
   ],
   "source": [
    "# Reattach ion info to test set\n",
    "X_test_df = X_test.reset_index(drop=True).copy()\n",
    "X_test_df['true'] = y_test.values\n",
    "X_test_df['pred'] = predictions.cpu().numpy().flatten()\n",
    "\n",
    "print(\"\\nPer-ion MAE on test set:\")\n",
    "for ion in ion_columns:\n",
    "    subset = X_test_df[X_test_df[ion] == 1]\n",
    "    if not subset.empty:\n",
    "        mae = np.mean(np.abs(subset['true'] - subset['pred']))\n",
    "        print(f\"{ion.replace('working_ion_', '')}: MAE = {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e90616-d93a-4f38-b8e5-47077d62a39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
